
# See linkchecker -h for help on these options.
# Commandline options override these settings.

##################### output configuration ##########################

[output]
# enable debug messages; see 'linkchecker -h' for valid debug names
#debug=all
# print status output
#status=1
# change the logging type
log=text
# turn on/off --verbose
verbose=1
# turn on/off --warnings
#warnings=0
# turn on/off --quiet
#quiet=1
complete=1
# additional file output
#fileoutput = text, html, gml, sql
fileoutput = xml

##################### logger configuration ##########################
#
# logger output part names:
#
# all       For all parts
# id        (a unique ID for each logentry)
# realurl   The full url link
# result    Valid or invalid, with messages
# extern    1 or 0, only in some logger types reported
# base      <base href=...>
# name      <a href=...>name</a> and <img alt="name">
# parenturl The referrer URL if there is any
# info      Some additional info, e.g. FTP welcome messages
# warning   Warnings
# dltime    Download time
# checktime Check time
# url       The original url name, can be relative
# intro     The blurb at the beginning, "starting at ..."
# outro     The blurb at the end, "found x errors ..."
# stats     Statistics including URL lengths and contents.

# each Logger can have separate configuration parameters

# custom xml logger
[xml]
encoding=UTF-8


##################### checking options ##########################

[checking]

# number of threads
threads=80

# connection timeout in seconds
timeout=30

# check anchors?
#anchors=1

# Accept and send HTTP cookies.
cookies=1

# Check recursively all links up to given depth. A negative depth will enable infinite recursion. Default depth is infinite. 
# recursionlevel=1
# supply a regular expression for which warnings are printed if found
# in any HTML files.
warningregex=(?:<title>)[^<]+(?:</title>)

# Basic NNTP server. Overrides NNTP_SERVER environment variable.
# warn if size info exceeds given maximum of bytes
#warnsizebytes=2000
#nntpserver=

# check HTML or CSS syntax with the W3C online validator
#checkhtml=1
#checkcss=1

# scan URL content for viruses with ClamAV
#scanvirus=1

# ClamAV config file
#clamavconf=/etc/clamav/clamd.conf

# Send and store cookies
#cookies=1

# parse a cookiefile for initial cookie data
#cookiefile=/path/to/cookies.txt

# User-Agent header string to send to HTTP web servers
#useragent=Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)
useragent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36

# Pause the given number of seconds between two subsequent connection
# requests to the same host.
#pause=1

# When checking finishes, write a memory dump to a temporary file.
# The memory dump is written both when checking finishes normally
# and when checking gets canceled.
# The memory dump only works if the python-meliae package is installed.
# Otherwise a warning is printed to install it.
#debugmemory=0

# When checking absolute URLs inside local files, the given root directory
# is used as base URL.
# Note that the given directory must have URL syntax, so it must use a slash
# to join directories instead of a backslash.
# And the given directory must end with a slash.
# Unix example:
#localwebroot=/var/www/
# Windows example:
#localwebroot=/C|/public_html/

# Check that SSL certificates are at least the given number of days valid.
# The number must not be negative.
# If the number of days is zero a warning is printed only for certificates
# that are already expired.
# The default number of days is 14.
#sslcertwarndays=14

# Stop checking new URLs after the given number of seconds. Same as if the
# user hits Ctrl-C after X seconds.
#maxrunseconds=60

# Maximum number of URLs to check. New URLs will not be queued after the
# given number of URLs is checked.
#maxnumurls=153

# Maximum number of connections to one single host for different connection types.
#maxconnectionshttps=10
#maxconnectionsftp=2
maxconnectionshttp=5

# On Linkchecker v9.x this is off by default
checkextern=1

##################### filtering options ##########################
#
# Only check syntax of URLs matching the given regular expressions.
#

[filtering]

ignore=
	
	#^http://atrium\.umontreal\.ca/primo-explore/
	#atrium\.umontreal\.ca/primo_library
	#^http://atrium\.umontreal\.ca
	^http://proxy\.umontreal\.ca
	#^http://calypso\.bib\.umontreal\.ca
	^http://docs\.bib\.umontreal\.ca
	^http://expo\.bib\.umontreal\.ca
	^http://guides\.bib\.umontreal\.ca/cours-impression/
	^http://guides\.bib\.umontreal\.ca/disciplines-impression/
	^http://maestro\.bib\.umontreal\.ca
	^http://opurl\.bib\.umontreal\.ca
	^https?://papyrus\.bib\.umontreal\.ca
	^http://permalien\.bib\.umontreal\.ca
	^http://salles\.bib\.umontreal\.ca
	^http://www\.bib\.umontreal\.ca/centenaire-Baby/
	^http://www\.bib\.umontreal\.ca/communiques
	^http://www\.bib\.umontreal\.ca/[Dd][Rr]/bulletin/
	^http://www\.bib\.umontreal\.ca/gif
	^http://www\.bib\.umontreal\.ca/[Ii]nfo[Ss]phere
	^http://www\.bib\.umontreal\.ca/modele/
	^https?://api\.bib\.umontreal\.ca
	
	# Adresses du bandeau UdeM
	^http://www\.umontreal\.ca/$
	^https://outlook\.umontreal\.ca
	^https?://jade\.daa\.umontreal\.ca
	^http://www\.umontreal\.ca/repertoires/
	^http://www\.umontreal\.ca/plancampus/
	^http://www\.umontreal\.ca/index/az\.html$
	^https://www\.portail\.umontreal\.ca/
	
	# Adresses du pied de page UdeM
	^http://(www\.)?urgence\.umontreal\.ca/$
	^http://www\.umontreal\.ca/vie-privee\.html$
	^http://www\.umontreal\.ca/offres-emploi\.html$
	^https://studium\.umontreal\.ca/$
	^http://itunesu\.umontreal\.ca/$
	
	# Adresses simplifiées
	^http://www\.bib\.umontreal\.ca/24\-7/?$
	^http://www\.bib\.umontreal\.ca/apprendre/?$
	^http://www\.bib\.umontreal\.ca/[Aa]trium/?$
	^http://www\.bib\.umontreal\.ca/[Bb][Cc][Ii]/?$
	^http://www\.bib\.umontreal\.ca/bib[Ee][Cc][Ll][Aa][Ii][Rr]/?$
	^http://www\.bib\.umontreal\.ca/calendrier/?$
	^http://www\.bib\.umontreal\.ca/[Cc]alypso/?$
	^http://www\.bib\.umontreal\.ca/citer/?$
	^http://www\.bib\.umontreal\.ca/dictionnaires/?$
	^http://www\.bib\.umontreal\.ca/droitdauteur/?$
	^http://www\.bib\.umontreal\.ca/expo[Gg]allimard/?$
	^http://www\.bib\.umontreal\.ca/formation/?$
	^http://www\.bib\.umontreal\.ca/geoindex/?$
	^http://www\.bib\.umontreal\.ca/guides/?$
	^http://www\.bib\.umontreal\.ca/[Ll]ank/?$
	^http://www\.bib\.umontreal\.ca/livraison/?$
	^http://www\.bib\.umontreal\.ca/[Mm]aestro/?$
	^http://www\.bib\.umontreal\.ca/mediatheque/?$
	^http://www\.bib\.umontreal\.ca/obtenir/?$
	^http://www\.bib\.umontreal\.ca/[Pp]apyrus/?$
	^http://www\.bib\.umontreal\.ca/[Pp][Ee][Bb]/?$
	^http://www\.bib\.umontreal\.ca/pret\-reseau/?$
	^http://www\.bib\.umontreal\.ca/[Pp]roxy/?$
	^http://www\.bib\.umontreal\.ca/quifaitquoi/?$
	^http://www\.bib\.umontreal\.ca/revues/?$
	^http://www\.bib\.umontreal\.ca/[Ss][Ii][Dd]/?$
	^http://www\.bib\.umontreal\.ca/tablette/?$
	^http://www\.bib\.umontreal\.ca/trouver/?$
	^http://www\.bib\.umontreal\.ca/une[Qq]uestion/?$
	^http://www\.bib\.umontreal\.ca/[Vv][Pp][Nn]/?$
	
	# Services de réduction d'URL
	^http://bit\.ly/
	^http://(www\.)?tinyurl\.com/
	^http://goo\.gl/
	
	# Autres liens à ne pas vérifier
	^http://guides\.bib\.umontreal\.ca/disciplines/339-Atrium
	^http://guides\.bib\.umontreal\.ca/dashboard
	^http://guides\.bib\.umontreal\.ca/login
	^https://identification\.umontreal\.ca
	ItunesU_accueil
	/images/iu/arrow\-left\.gif
	/images/iu/ligne\-pied\-page\.png
	^http://www\.adobe\.com/go/ # Redirections Adobe
	^http://go\.microsoft\.com  # Redirections Microsoft
	purl\.org
	\.js\# #Liens vers des fichiers javascript avec paramètres passés avec la portion 'hash' de l'URL
	^http://nouveauxetudiants\.umontreal\.ca # Linkchecker v7.9 interprète mal la règle du fichier robots.txt de ce site et croie qu'il n'a pas le droit d'y entrer.
	^http://www\.pons\.eu
	^http://www\.westlaw\.com/search/default\.wl
	^http://web\.nexis\.com/sources/scripts/eslClient\.pl

# Check but do not recurse into URLs matching the given regular expressions.

noFollow=
	^http://atrium\.umontreal\.ca
	^http://primo-test\.bib\.umontreal\.ca
	^http://bibres\.bib\.umontreal\.ca
	^http://pds\.bib\.umontreal\.ca
	^http://dx\.doi\.org
	^http://permalien\.bib\.umontreal\.ca
	^http://www\.bib\.umontreal\.ca/communiques
	^http://calypso\.bib\.umontreal\.ca

# Ignore specified warnings (see linkchecker -h for the list of
# recognized warnings). Add a comma-separated list of warnings here
# that prevent a valid URL from being logged. Note that the warning
# will be logged in invalid URLs.
ignorewarnings=http-robots-denied,file-missing-slash,url-unnormed,url-unicode-domain,url-anchor-not-found,http-cookie-store-error,url-content-duplicate,https-certificate-error

# Regular expression to add more URLs recognized as internal links.
# Default is that URLs given on the command line are internal.
internlinks=^http://.+\.bib\.umontreal\.ca|^http://atrium\.umontreal\.ca


##################### password authentication ##########################

[authentication]

# WARNING: if you store passwords in this configuration entry, make sure the
# configuration file is not readable by other users.
# Different user/password pairs for different URLs can be provided.
# Entries are a triple (URL regular expression, username, password),
# separated by whitespace.
# If the regular expression matches, the given user/password pair is used
# for authentication. The commandline options -u,-p match every link
# and therefore override the entries given here. The first match wins.
# At the moment, authentication is used for http[s] and ftp links.
#entry=
# Note that passwords are optional. If any passwords are stored here,
# this file should not readable by other users.
#  ^https?://www\.example\.com/~calvin/ calvin mypass
#  ^ftp://www\.example\.com/secret/ calvin

# if the website requires a login the URL and optionally the user and
# password CGI fieldnames can be provided.
#loginurl=http://www.example.com/

# The name of the user and password CGI field
#loginuserfield=login
#loginpasswordfield=password
# Optionally any additional CGI name/value pairs. Note that the default
# values are submitted automatically.
#loginextrafields=
#  name1:value1
#  name 2:value 2
